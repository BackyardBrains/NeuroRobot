
filename = 'mdp_'
load(filename)

%%
env = rlMDPEnv(mdp);
% env.ResetFcn = @() 24;
validateEnvironment(env)
obsInfo = getObservationInfo(env);
actInfo = getActionInfo(env);
qTable = rlTable(obsInfo, actInfo);
critic = rlQValueFunction(qTable,obsInfo,actInfo);

%% Agent 1 (Q)
agent_opt = rlQAgentOptions;
qOptions = rlOptimizerOptions;
% qOptions.LearnRate = 0.1;
agentOpts.CriticOptimizerOptions = qOptions;
agent = rlQAgent(critic, agent_opt);
training_opts = rlTrainingOptions;
training_opts.MaxEpisodes = 500;
training_opts.MaxStepsPerEpisode = 100;
training_opts.StopTrainingValue = 1000;
training_opts.StopTrainingCriteria = "AverageReward";
training_opts.ScoreAveragingWindowLength = 5;
trainingStats_shallow = train(agent,env, training_opts);

%%
figure(11)
clf
set(gcf, 'color', 'w')
scan_agent
ylim([0 n_unique_states + 1])
title('mdp_rand')
set(gca, 'xtick', [], 'ytick', [], 'xcolor', 'w', 'ycolor', 'w')
export_fig(horzcat('agent1_', 'mdp_rand', '_net'), '-r150', '-jpg', '-nocrop')
save(horzcat('agent1_', 'mdp_rand'), 'agent')

% %% Agent 2 (Deep Q)
% agent_opt = rlDQNAgentOptions;
% agent_opt.DiscountFactor = 0.99;
% % agent_opt.EpsilonGreedyExploration.Epsilon = 0.1;
% % agent_opt.EpsilonGreedyExploration.EpsilonMin = 0.01;
% % agent_opt.EpsilonGreedyExploration.EpsilonDecay = 0.5;
% agent = rlDQNAgent(critic, agent_opt);
% training_opts = rlTrainingOptions;
% training_opts.MaxEpisodes = 100;
% training_opts.MaxStepsPerEpisode = 20;
% training_opts.StopTrainingValue = 500000;
% training_opts.StopTrainingCriteria = "AverageReward";
% training_opts.ScoreAveragingWindowLength = 5;
% training_opts.UseParallel = 1;
% trainingStats_deep = train(agent, env, training_opts);
% figure(12)
% clf
% set(gcf, 'color', 'w')
% scan_agent
% ylim([0 n_unique_states + 1])
% title('Agent 2')
% set(gca, 'xtick', [], 'ytick', [], 'xcolor', 'w', 'ycolor', 'w')
% export_fig(horzcat('agent2_', num2str(date), '_net'), '-r150', '-jpg', '-nocrop')
% save('agent2', 'agent')

% %% Agent 3 (SARSA)
% critic = rlQValueFunction(qTable,obsInfo,actInfo);
% opt = rlSARSAAgentOptions
% agent = rlSARSAAgent(critic,opt)
% training_opts = rlTrainingOptions;
% training_opts.MaxEpisodes = 500;
% training_opts.MaxStepsPerEpisode = 100;
% training_opts.StopTrainingValue = 500000;
% training_opts.StopTrainingCriteria = "AverageReward";
% training_opts.ScoreAveragingWindowLength = 5;
% trainingStats_shallow = train(agent,env, training_opts);
% figure(11)
% clf
% set(gcf, 'color', 'w')
% scan_agent
% ylim([0 n_unique_states + 1])
% title('Agent 3 (sarsa) randomized tuples')
% set(gca, 'xtick', [], 'ytick', [], 'xcolor', 'w', 'ycolor', 'w')
% export_fig(horzcat('agent3_rand_', num2str(date), '_net'), '-r150', '-jpg', '-nocrop')
% % save('agent3', 'agent')
% 
% 
% %% Agent 4 (custom)
% reinforcementLearningDesigner
% 
% figure(14)
% clf
% set(gcf, 'color', 'w')
% scan_agent
% ylim([0 n_unique_states + 1])
% title('Agent 4 (custom) randomized tuples')
% set(gca, 'xtick', [], 'ytick', [], 'xcolor', 'w', 'ycolor', 'w')
% export_fig(horzcat('agent4_rand_', num2str(date), '_net'), '-r150', '-jpg', '-nocrop')
% % save('agent4', 'agent')


