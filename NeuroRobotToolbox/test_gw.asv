


gw = createGridWorld(5,5);
nS = numel(gw.States);
nA = numel(gw.Actions);
gw.R = -1*ones(nS,nS,nA);
gw.R(25, 25, :) = 10;
env = rlMDPEnv(gw);

validateEnvironment(env)
obsInfo = getObservationInfo(env);
actInfo = getActionInfo(env);
qTable = rlTable(obsInfo, actInfo);
critic = rlQValueFunction(qTable,obsInfo,actInfo);

% load('basicGWQAgent.mat','qAgent')
% agent = qAgent;

agent_opt = rlQAgentOptions;
agent = rlQAgent(critic, agent_opt);

training_opts = rlTrainingOptions;
training_opts.MaxEpisodes = 100;
training_opts.MaxStepsPerEpisode = 50;
training_opts.StopTrainingValue = 1000;
training_opts.StopTrainingCriteria = "AverageReward";
training_opts.ScoreAveragingWindowLength = 5;
trainingStats_shallow = train(agent,env, training_opts);
